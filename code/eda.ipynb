{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 数据质量分析\n",
    "* 命令行运行时，切换工作目录\n",
    "\n",
    "```\n",
    "import os\n",
    "os.chdir('./code')\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import thinkstats2\n",
    "import math\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "def conditional_entropy(x, cond, flag='flag'):\n",
    "    \"\"\"\n",
    "    :var x : Pandas.DataFrame\n",
    "    \"\"\"\n",
    "    res = 0\n",
    "    column_res = {}\n",
    "\n",
    "    cond_column = x[cond].dropna()\n",
    "    for v in cond_column.unique():\n",
    "        part = x[x[cond]==v]\n",
    "        part_count = part.shape[0]\n",
    "        part_entropy = []\n",
    "        for c in part.groupby(flag)[flag].count():\n",
    "            part_entropy.append( (c-0.)/part_count)\n",
    "        column_res[v] = entropy(part_entropy),part_count\n",
    "        res += (part_count-0.) / cond_column.shape[0] * column_res[v][0]\n",
    "\n",
    "    return res,column_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_at = pd.read_csv('../data/A_train.csv')\n",
    "data_bt = pd.read_csv('../data/B_train.csv')\n",
    "data_test = pd.read_csv('../data/B_test.csv')\n",
    "target_columns = [] #目标维度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 维度情况\n",
    "    * 3组数据维度一样，维度之间可比较？"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(491L,)\n",
      "(491L,)\n",
      "(490L,)\n"
     ]
    }
   ],
   "source": [
    "print data_bt.columns.shape\n",
    "print data_bt.columns.intersection(data_at.columns).shape\n",
    "print data_bt.columns.intersection(data_test.columns).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 缺失值分析\n",
    "    * https://blog.csdn.net/weixin_40159138/article/details/89421014\n",
    "    * https://www.jianshu.com/p/9c867fb9cf17\n",
    "    * https://scikit-learn.org/stable/modules/impute.html#impute\n",
    "    * 从下图可以看到缺失值非常接近\n",
    "    * B_test和A_train缺失值情况几乎一致\n",
    "    * B_train的缺失值情况比A_train严重很多\n",
    "    * A_train中20%的用户缺失维度在100个以内，40%的缺失维度在450个以上，60%用户的缺失维度在150个以内\n",
    "    * B_train 和 B_test 38%的用户缺失维度在186左右，60%用户维度缺失在460以上\n",
    "    * 缺失值在学习过程中，feature_importance 会降低，并不一定会影响学习效果，*可以对比是否填充缺失值对结果的影响*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fea_null = np.sum(data_at.isnull(), axis=0)\n",
    "feb_null = np.sum(data_bt.isnull(), axis=0)\n",
    "fet_null = np.sum(data_test.isnull(), axis=0)\n",
    "\n",
    "plt.subplot(311).plot(fea_null.values)\n",
    "plt.subplot(312).plot(feb_null.values)\n",
    "plt.subplot(313).plot(fet_null.values)\n",
    "\n",
    "# sort_values\n",
    "plt.subplot(311).plot(np.sort(fea_null))\n",
    "plt.subplot(312).plot(np.sort(feb_null))\n",
    "plt.subplot(313).plot(np.sort(fet_null))\n",
    "plt.show()\n",
    "\n",
    "# 缺失值归一化\n",
    "plt.plot(np.sort(fea_null/data_at.shape[0]), color='green')\n",
    "plt.plot(np.sort(feb_null/data_bt.shape[0]), color='blue')\n",
    "plt.plot(np.sort(fet_null/data_test.shape[0]),color='red')\n",
    "plt.show()\n",
    "\n",
    "u_fea_null = np.sum(data_at.isnull(), axis=1)\n",
    "u_feb_null = np.sum(data_bt.isnull(), axis=1)\n",
    "u_fet_null = np.sum(data_test.isnull(), axis=1)\n",
    "u_fea_null.hist(cumulative=True, density=1, bins=100, alpha=.2, color=\"r\")\n",
    "u_feb_null.hist(cumulative=True, density=1, bins=100, alpha=.2, color=\"b\")\n",
    "u_fet_null.hist(cumulative=True, density=1, bins=100, alpha=.2, color=\"g\")\n",
    "plt.show()\n",
    "print \"用户的维度缺失情况\"\n",
    "print (u_fea_null[u_fea_null<156].count()-0.0)/u_fea_null.count()\n",
    "print (u_feb_null[u_feb_null<186].count()-0.0)/u_feb_null.count()\n",
    "print (u_fet_null[u_fet_null<186].count()-0.0)/u_fet_null.count()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 连续与离散\n",
    "    * 知识：https://blog.csdn.net/ztf312/article/details/53991329 https://blog.csdn.net/banbuduoyujian/article/details/53957653\n",
    "    * https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization.html https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.cut.html\n",
    "    * 489维数据中，383维的取值在100个以内，推测大部分应该是离散值\n",
    "    * 取值在100个以上的可能为连续值\n",
    "    * 取值大于300的维度81个，大于500的69个，这些极可能是连续值\n",
    "    * 需要对连续值进行离散化：分类"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "discrete_threshold = 120\n",
    "data_tmp_all = pd.concat([data_at.sort_index(axis=1).drop(['flag','no'],axis=1),data_bt.sort_index(axis=1).drop(['flag','no'],axis=1)])\n",
    "data_tmp_all = pd.concat([data_tmp_all, data_test.sort_index(axis=1).drop(['no'],axis=1)])\n",
    "data_all_n_unique = data_tmp_all.nunique()\n",
    "#plt.hist(data_all_n_unique, cumulative=False, bins=100)\n",
    "#plt.show()\n",
    "discrete_columns = data_all_n_unique[data_all_n_unique<=discrete_threshold].index\n",
    "continuous_columns = data_all_n_unique[data_all_n_unique>discrete_threshold].index\n",
    "null_columns_all = np.sum(data_tmp_all.isnull(), axis=0).sort_values(ascending=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 异常值分析\n",
    "    * https://www.cnblogs.com/tinglele527/p/11955103.html\n",
    "    * https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "    * https://scikit-learn.org/0.20/auto_examples/plot_anomaly_comparison.html\n",
    "    * https://blog.csdn.net/PbGc396Dwxjb77F2je/article/details/99687952\n",
    "    * 离散值中取值比例很小的这部分，可能有两种情况：对预测结果有强作用，对预测情况无影响，*可以做对比分析*\n",
    "    * 离散值中，取值比例很小的部分，如果熵很大，说明本身对结果没有区分度，这部分异常值可能性很大，\n",
    "    * 离散值中，条件熵大的维度，区分度小，这部分维度可能需要去除掉\n",
    "    * 连续值中，box plot可以很方便观测处异常值\n",
    "    * 连续值在A_train中异常值偏少，整体少于4%，是否需要处理？连续值在B_train中，整体异常值少于2.5%\n",
    "    *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#标准差法 sunspots.counts > xbar + 2 * xstd\n",
    "data_at_continuous = data_at[continuous_columns]\n",
    "data_at_cont_abnormal = data_at_continuous > data_at_continuous.mean()+2*data_at_continuous.std()\n",
    "data_at_cont_ab_proportion = np.sum(data_at_cont_abnormal, axis=0)/data_at_cont_abnormal.shape[0]\n",
    "plt.hist(data_at_cont_ab_proportion, cumulative=True, bins=100, density=True);plt.show()\n",
    "\n",
    "data_bt_continuous = data_bt[continuous_columns]\n",
    "data_bt_cont_abnormal = data_bt_continuous > data_bt_continuous.mean()+2*data_bt_continuous.std()\n",
    "data_bt_cont_ab_proportion = np.sum(data_bt_cont_abnormal, axis=0)/data_bt_cont_abnormal.shape[0]\n",
    "plt.hist(data_bt_cont_ab_proportion, cumulative=True, bins=100, density=True);plt.show()\n",
    "\n",
    "# 维度与熵\n",
    "entropy_dis = {}\n",
    "count_threshold = data_at.shape[0] * 0.05\n",
    "entropy_threshold = 0.5\n",
    "# 每个维度的异常熵\n",
    "least_count_entropy = {}\n",
    "for column in discrete_columns:\n",
    "    s,d = conditional_entropy(data_at, cond=column)\n",
    "    entropy_dis[column] = s\n",
    "    for v in d:\n",
    "        # 离散值中，如果某个值的数量很少，单独存起来\n",
    "        if d[v][1]<=count_threshold and d[v][0]>=entropy_threshold:\n",
    "            if column not in least_count_entropy:\n",
    "                least_count_entropy[column] = {}\n",
    "            least_count_entropy[column][v] = d[v]\n",
    "entropy_dis = pd.DataFrame.from_dict(entropy_dis, orient='index')\n",
    "plt.hist(np.sort(entropy_dis, axis=0), bins=100, cumulative=True);plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 同分布检验\n",
    "    * https://blog.csdn.net/qq_41679006/article/details/80977113\n",
    "    * https://www.cnblogs.com/arkenstone/p/5496761.html\n",
    "    * https://blog.csdn.net/t15600624671/article/details/78770239\n",
    "    * B_test 和 B_train只有2维数据的分布差异较大，显著性α=0.05\n",
    "    * B_train 和 A_train的数据差异较大：有199维数据的分布相差大，所以考虑剔除掉199维数据\n",
    "    * 缺失值少，且同分布的维度 极有可能是最重要的维度，可以尝试只取这部分数据进行分析， *可以做对比分析*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dis_b_diff = {};dis_b_same={};dis_ab_diff={};dis_ab_same={}\n",
    "for column in data_bt.columns.drop(['no','flag']):\n",
    "    print column\n",
    "    d, p = ks_2samp(data_bt[column].dropna(), data_test[column].dropna())\n",
    "    if p<=0.05:\n",
    "        dis_b_diff[column] = (d,p)\n",
    "    else:\n",
    "        dis_b_same[column] = (d,p)\n",
    "    d, p = ks_2samp(data_bt[column].dropna(), data_at[column].dropna())\n",
    "    if p<=0.05:\n",
    "        dis_ab_diff[column] = (d,p)\n",
    "    else:\n",
    "        dis_ab_same[column] = (d,p)\n",
    "dis_b_diff = pd.DataFrame.from_dict(dis_b_diff,orient='index')\n",
    "dis_b_same = pd.DataFrame.from_dict(dis_b_same,orient='index')\n",
    "dis_ab_diff = pd.DataFrame.from_dict(dis_ab_diff,orient='index')\n",
    "dis_ab_same = pd.DataFrame.from_dict(dis_ab_same,orient='index')\n",
    "# 缺失值少，且同分布的维度\n",
    "null_num = 300\n",
    "same_features = feb_null.sort_values(ascending=False).tail(null_num).index.intersection(dis_ab_same.index)\n",
    "same_features = same_features.intersection(fea_null.sort_values(ascending=False).tail(null_num).index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 数据规范化\n",
    "    * https://blog.csdn.net/weixin_38706928/article/details/80329563\n",
    "    * https://scikit-learn.org/stable/modules/preprocessing.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 经过以上分析，可以分布验证数据的处理情况\n",
    "    * 排除null值多的维度\n",
    "    * 取A B同分布维度，B_test + B_train = B\n",
    "    * 剔除线性相关性强的维度\n",
    "    * 数据离散化：sklearn\n",
    "    * 排除条件熵大的维度\n",
    "    * 填充null值：固定填充，根据分布填充\n",
    "    * 数据规范化\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "# coding=utf-8\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}