{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 数据质量分析\n",
    "* 命令行运行时，切换工作目录\n",
    "\n",
    "```\n",
    "import os\n",
    "os.chdir('./code')\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import thinkstats2\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn import metrics\n",
    "from eda_kit import conditional_entropy,roc_auc_score,auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "A_train = pd.read_csv('../data/A_train.csv')\n",
    "B_train = pd.read_csv('../data/B_train.csv')\n",
    "B_test = pd.read_csv('../data/B_test.csv')\n",
    "\n",
    "data_at = A_train\n",
    "data_bt = B_train\n",
    "data_test = B_test\n",
    "\n",
    "print (data_at.dtypes[data_at.dtypes == np.int64])\n",
    "print (data_bt.dtypes[data_bt.dtypes == np.int64])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 特殊的一个字段\n",
    "data_bt['UserInfo_170'].unique()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_bt = data_bt.drop(['UserInfo_170'], axis=1)\n",
    "data_at = data_at.drop(['UserInfo_170'], axis=1)\n",
    "data_test = data_test.drop(['UserInfo_170'], axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 维度情况\n",
    "    * 3组数据维度一样，维度之间可比较？"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(491L,)\n",
      "(491L,)\n",
      "(490L,)\n"
     ]
    }
   ],
   "source": [
    "print (data_bt.columns.shape)\n",
    "print (data_bt.columns.intersection(data_at.columns).shape)\n",
    "print (data_bt.columns.intersection(data_test.columns).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 缺失值分析\n",
    "    * https://blog.csdn.net/weixin_40159138/article/details/89421014\n",
    "    * https://www.jianshu.com/p/9c867fb9cf17\n",
    "    * https://scikit-learn.org/stable/modules/impute.html#impute\n",
    "    * https://blog.csdn.net/qq_38958113/article/details/98220246\n",
    "    * 从下图可以看到缺失值非常接近\n",
    "    * B_test和A_train缺失值情况几乎一致\n",
    "    * B_train的缺失值情况比A_train严重很多\n",
    "    * A_train中20%的用户缺失维度在100个以内，40%的缺失维度在450个以上，60%用户的缺失维度在150个以内\n",
    "    * B_train 和 B_test 38%的用户缺失维度在186左右，60%用户维度缺失在460以上\n",
    "    * 缺失值在学习过程中，feature_importance 会降低，并不一定会影响学习效果，*可以对比是否填充缺失值对结果的影响*\n",
    "    * 填充策略：\n",
    "        * 离散值填充\n",
    "        * 连续值填充\n",
    "        * A的情况是正确的情况下，可以用于填充B的值，或者AB的分布，可以填充B_test的值\n",
    "        * 对待缺失值不同的态度，为决定不同的填充策略：比如用户主动不填收入，那么这个null值应该赋予一个已有值之外的，比如特殊的-999.9\n",
    "        * 对A值采取固定填充，但是可以对B采取transform的填充，\n",
    "        * 填充时机应该延后"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fea_null = np.sum(data_at.isnull(), axis=0)\n",
    "feb_null = np.sum(data_bt.isnull(), axis=0)\n",
    "fet_null = np.sum(data_test.isnull(), axis=0)\n",
    "\n",
    "plt.subplot(311).plot(fea_null.values)\n",
    "plt.subplot(312).plot(feb_null.values)\n",
    "plt.subplot(313).plot(fet_null.values)\n",
    "\n",
    "# sort_values\n",
    "plt.subplot(311).plot(np.sort(fea_null))\n",
    "plt.subplot(312).plot(np.sort(feb_null))\n",
    "plt.subplot(313).plot(np.sort(fet_null))\n",
    "plt.show()\n",
    "\n",
    "# 缺失值归一化\n",
    "plt.plot(np.sort(fea_null/data_at.shape[0]), color='green')\n",
    "plt.plot(np.sort(feb_null/data_bt.shape[0]), color='blue')\n",
    "plt.plot(np.sort(fet_null/data_test.shape[0]),color='red')\n",
    "plt.show()\n",
    "\n",
    "u_fea_null = np.sum(data_at.isnull(), axis=1)\n",
    "u_feb_null = np.sum(data_bt.isnull(), axis=1)\n",
    "u_fet_null = np.sum(data_test.isnull(), axis=1)\n",
    "u_fea_null.hist(cumulative=True, density=1, bins=100, alpha=.2, color=\"r\")\n",
    "u_feb_null.hist(cumulative=True, density=1, bins=100, alpha=.2, color=\"b\")\n",
    "u_fet_null.hist(cumulative=True, density=1, bins=100, alpha=.2, color=\"g\")\n",
    "plt.show()\n",
    "print( \"用户的维度缺失情况\")\n",
    "print( (u_fea_null[u_fea_null<156].count()-0.0)/u_fea_null.count())\n",
    "print( (u_feb_null[u_feb_null<186].count()-0.0)/u_feb_null.count())\n",
    "print( (u_fet_null[u_fet_null<186].count()-0.0)/u_fet_null.count())\n",
    "\n",
    "# preserve less null columns\n",
    "threshold_column_null = 0.99\n",
    "at_target_columns = fea_null[fea_null<data_at.shape[0]*threshold_column_null].sort_values(axis=0).index\n",
    "bt_target_columns = feb_null[feb_null<data_bt.shape[0]*threshold_column_null].sort_values(axis=0).index\n",
    "test_target_columns = fet_null[fet_null<data_test.shape[0]*threshold_column_null].sort_values(axis=0).index\n",
    "target_columns = at_target_columns.intersection(bt_target_columns)\n",
    "data_at = data_at.fillna(10)\n",
    "data_bt = data_bt.fillna(10)\n",
    "data_test = data_test.fillna(10)\n",
    "#data_at = data_at[target_columns]\n",
    "#data_bt = data_bt[target_columns]\n",
    "#data_test = data_test[target_columns.drop(['flag'])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 连续与离散\n",
    "    * 知识：https://blog.csdn.net/ztf312/article/details/53991329\n",
    "    * https://blog.csdn.net/banbuduoyujian/article/details/53957653\n",
    "    * https://scikit-learn.org/stable/auto_examples/preprocessing/plot_discretization.html\n",
    "    * https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.cut.html\n",
    "    * https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "    * https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-discretization\n",
    "    * 489维数据中，383维的取值在100个以内，推测大部分应该是离散值\n",
    "    * 取值在100个以上的可能为连续值\n",
    "    * 取值大于300的维度81个，大于500的69个，这些极可能是连续值\n",
    "    * 需要对连续值进行离散化：分类"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "discrete_threshold = 120\n",
    "data_tmp_all = pd.concat([data_at.sort_index(axis=1).drop(['flag','no'],axis=1),data_bt.sort_index(axis=1).drop(['flag','no'],axis=1)])\n",
    "data_tmp_all = pd.concat([data_tmp_all, data_test.sort_index(axis=1).drop(['no'],axis=1)])\n",
    "data_all_n_unique = data_tmp_all.nunique()\n",
    "#plt.hist(data_all_n_unique, cumulative=False, bins=100)\n",
    "#plt.show()\n",
    "discrete_columns = data_all_n_unique[data_all_n_unique<=discrete_threshold].index\n",
    "continuous_columns = data_all_n_unique[data_all_n_unique>discrete_threshold].index\n",
    "null_columns_all = np.sum(data_tmp_all.isnull(), axis=0).sort_values(ascending=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 同分布检验\n",
    "    * https://blog.csdn.net/qq_41679006/article/details/80977113\n",
    "    * https://www.cnblogs.com/arkenstone/p/5496761.html\n",
    "    * https://blog.csdn.net/t15600624671/article/details/78770239\n",
    "    * B_test 和 B_train只有2维数据的分布差异较大，显著性α=0.05\n",
    "    * B_train 和 A_train的数据差异较大：有199维数据的分布相差大，所以考虑剔除掉199维数据\n",
    "    * 缺失值少，且同分布的维度 极有可能是最重要的维度，可以尝试只取这部分数据进行分析， *可以做对比分析*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_at = A_train\n",
    "data_bt = B_train\n",
    "data_test = B_test\n",
    "#data_at[continuous_columns] = data_at[continuous_columns].fillna(-999)\n",
    "#data_bt[continuous_columns] = data_bt[continuous_columns].fillna(-999)\n",
    "#data_test[continuous_columns] = data_test[continuous_columns].fillna(-999)\n",
    "dis_b_diff = {};dis_b_same={};dis_ab_diff={};dis_ab_same={};dis_abt_diff={};dis_abt_same={}\n",
    "for column in target_columns.intersection(continuous_columns):\n",
    "    # print column\n",
    "    d, p = ks_2samp(data_bt[column].dropna(), data_test[column].dropna())\n",
    "    if p<=0.05:\n",
    "        dis_b_diff[column] = (d,p)\n",
    "    else:\n",
    "        dis_b_same[column] = (d,p)\n",
    "    d, p = ks_2samp(data_bt[column].dropna(), data_at[column].dropna())\n",
    "    if p<=0.05:\n",
    "        dis_ab_diff[column] = (d,p)\n",
    "    else:\n",
    "        dis_ab_same[column] = (d,p)\n",
    "    d, p = ks_2samp(data_test[column].dropna(), data_at[column].dropna())\n",
    "    if p<=0.05:\n",
    "        dis_abt_diff[column] = (d,p)\n",
    "    else:\n",
    "        dis_abt_same[column] = (d,p)\n",
    "dis_b_diff = pd.DataFrame.from_dict(dis_b_diff,orient='index')\n",
    "dis_b_same = pd.DataFrame.from_dict(dis_b_same,orient='index')\n",
    "dis_ab_diff = pd.DataFrame.from_dict(dis_ab_diff,orient='index')\n",
    "dis_ab_same = pd.DataFrame.from_dict(dis_ab_same,orient='index')\n",
    "# 缺失值少，且同分布的维度\n",
    "\n",
    "target_columns = target_columns.intersection(dis_ab_same.index).append(pd.Index(['no','flag']))\n",
    "data_at = data_at[target_columns]\n",
    "data_bt = data_bt[target_columns]\n",
    "data_test = data_test[target_columns.drop(['flag'])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 线性相关性\n",
    "    * 剔除掉线性相关性大的维度\n",
    "    * https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold_corr_bt = 0.98\n",
    "corr_bt = data_bt.corr()\n",
    "#sns.heatmap(corr_bt, annot=False, cmap=plt.cm.Reds)\n",
    "#plt.show()\n",
    "corr_length = corr_bt.shape[0]\n",
    "final_cols_bt = []\n",
    "del_cols_bt =[]\n",
    "for i in range(corr_length):\n",
    "    if corr_bt.columns[i] not in del_cols_bt:\n",
    "        final_cols_bt.append(corr_bt.columns[i])\n",
    "        for j in range(i+1,corr_length):\n",
    "            if (corr_bt.iloc[i,j] > threshold_corr_bt) and (corr_bt.columns[j] not in del_cols_bt):\n",
    "                del_cols_bt.append(corr_bt.columns[j])\n",
    "target_no_corr_columns = target_columns.intersection(pd.Index(final_cols_bt))\n",
    "target_no_corr_continuous_columns = target_no_corr_columns.intersection(continuous_columns)\n",
    "target_no_corr_discrete_columns = target_no_corr_columns.intersection(discrete_columns)\n",
    "data_at = data_at[target_columns]\n",
    "data_bt = data_bt[target_columns]\n",
    "data_test = data_test[target_columns.drop(['flag'])]\n",
    "sns.heatmap(data_bt.corr(), annot=False, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 再次离散化\n",
    "    * 首先填充null值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(data_at[continuous_columns])\n",
    "data_at[continuous_columns] = pd.DataFrame(imp_mean.transform(data_at[continuous_columns]),columns=continuous_columns)\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit(data_at[continuous_columns])\n",
    "data_bt[continuous_columns] = pd.DataFrame(data=imp.transform(data_bt[continuous_columns]),columns=continuous_columns)\n",
    "data_test['flag'] = np.nan\n",
    "data_test[continuous_columns] = pd.DataFrame(data=imp.transform(data_test[continuous_columns]),columns=continuous_columns)\n",
    "data_at[discrete_columns] = data_at[discrete_columns].fillna(1)\n",
    "data_bt[discrete_columns] = data_bt[discrete_columns].fillna(1)\n",
    "data_test[discrete_columns] = data_test[discrete_columns].fillna(1)\n",
    "data_at[discrete_columns] = data_at[discrete_columns].fillna(data_at[discrete_columns].mean())\n",
    "data_bt[discrete_columns] = data_bt[discrete_columns].fillna(data_at[discrete_columns].mean())\n",
    "data_test[discrete_columns] = data_test[discrete_columns].fillna(data_at[discrete_columns].mean())\n",
    "\n",
    "threshold_k_bins = 0.1\n",
    "est = KBinsDiscretizer(n_bins=np.round((data_at[continuous_columns].max()-data_at[continuous_columns].min())*threshold_k_bins).values, encode='ordinal', strategy='uniform').fit(data_at[continuous_columns])\n",
    "data_at.update( pd.DataFrame(est.transform(data_at[continuous_columns]),columns=continuous_columns))\n",
    "data_bt.update( pd.DataFrame(est.transform(data_bt[continuous_columns]),columns=continuous_columns))\n",
    "data_test.update( pd.DataFrame(est.transform(data_test[continuous_columns]),columns=continuous_columns))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 异常值分析\n",
    "    * https://www.cnblogs.com/tinglele527/p/11955103.html\n",
    "    * https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "    * https://scikit-learn.org/0.20/auto_examples/plot_anomaly_comparison.html\n",
    "    * https://blog.csdn.net/PbGc396Dwxjb77F2je/article/details/99687952\n",
    "    * 离散值中取值比例很小的这部分，可能有两种情况：对预测结果有强作用，对预测情况无影响，*可以做对比分析*\n",
    "    * 离散值中，取值比例很小的部分，如果熵很大，说明本身对结果没有区分度，这部分异常值可能性很大，\n",
    "    * 离散值中，条件熵大的维度，区分度小，这部分维度可能需要去除掉\n",
    "    * 经过前面的同分布处理后，条件熵减少了很多，\n",
    "    * 连续值中，box plot可以很方便观测处异常值\n",
    "    * 连续值在A_train中异常值偏少，整体少于4%，是否需要处理？连续值在B_train中，整体异常值少于2.5%\n",
    "    *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#标准差法 sunspots.counts > xbar + 2 * xstd\n",
    "data_at_continuous = data_at[continuous_columns]\n",
    "data_at_cont_abnormal = data_at_continuous > data_at_continuous.mean()+2*data_at_continuous.std()\n",
    "data_at_cont_ab_proportion = np.sum(data_at_cont_abnormal, axis=0)/data_at_cont_abnormal.shape[0]\n",
    "plt.hist(data_at_cont_ab_proportion, cumulative=True, bins=100, density=True);plt.show()\n",
    "\n",
    "data_bt_continuous = data_bt[continuous_columns]\n",
    "data_bt_cont_abnormal = data_bt_continuous > data_bt_continuous.mean()+2*data_bt_continuous.std()\n",
    "data_bt_cont_ab_proportion = np.sum(data_bt_cont_abnormal, axis=0)/data_bt_cont_abnormal.shape[0]\n",
    "plt.hist(data_bt_cont_ab_proportion, cumulative=True, bins=100, density=True);plt.show()\n",
    "\n",
    "# 维度与熵\n",
    "entropy_dis = {}\n",
    "count_threshold = data_at.shape[0] * 0.05\n",
    "entropy_threshold = 0.5\n",
    "# 每个维度的异常熵\n",
    "least_count_entropy = {}\n",
    "for column in discrete_columns:\n",
    "    s,d = conditional_entropy(data_at, cond=column)\n",
    "    entropy_dis[column] = s\n",
    "    for v in d:\n",
    "        # 离散值中，如果某个值的数量很少，单独存起来\n",
    "        if d[v][1]<=count_threshold and d[v][0]>=entropy_threshold:\n",
    "            if column not in least_count_entropy:\n",
    "                least_count_entropy[column] = {}\n",
    "            least_count_entropy[column][v] = d[v]\n",
    "entropy_dis = pd.DataFrame.from_dict(entropy_dis, orient='index')\n",
    "print (entropy_dis.max(),entropy_dis.min())\n",
    "plt.hist(np.sort(entropy_dis, axis=0), bins=100, cumulative=True);plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 数据规范化\n",
    "    * https://blog.csdn.net/weixin_38706928/article/details/80329563\n",
    "    * https://scikit-learn.org/stable/modules/preprocessing.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# stds = StandardScaler().fit(data_at.drop(['no','flag']))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 经过以上分析，可以分布验证数据的处理情况\n",
    "    * 排除null值多的维度\n",
    "    * 取A B同分布维度，B_test + B_train = B\n",
    "    * 剔除线性相关性强的维度\n",
    "    * 数据离散化：sklearn\n",
    "    * 排除条件熵大的维度\n",
    "    * 填充null值：固定填充，根据分布填充\n",
    "    * 数据规范化\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# XGB_B\n",
    "target_bb = data_bt['flag']\n",
    "\n",
    "bb_data = data_bt.drop(['flag','no'],axis=1)\n",
    "bbt_data = data_test.drop(['no','flag'], axis=1)\n",
    "\n",
    "#   GBDT训练 输出（47%以上的特征）   ，训练集划分交叉验证\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bb_data, target_bb, test_size=0.3, random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1,  max_depth=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pro = clf.predict_proba(X_test)\n",
    "y_prd = pd.DataFrame(y_pro).iloc[:,1]\n",
    "roc_auc_score(y_test,y_prd)\n",
    "\n",
    "# 训练所有数据 输出\n",
    "\n",
    "clf.fit(bb_data,target_bb)\n",
    "b = clf.predict_proba(bbt_data)\n",
    "b=pd.DataFrame(b)\n",
    "pro_b = b.iloc[:,1]\n",
    "no = data_test.iloc[:,-2]\n",
    "pro = pd.DataFrame(pro_b)\n",
    "no = pd.DataFrame(no)\n",
    "\n",
    "\n",
    "# GBDT 重要特征\n",
    "\n",
    "clf.fit(bb_data,target_bb)\n",
    "clf_importance = clf.feature_importances_\n",
    "clf_importance_ = pd.DataFrame(clf_importance)\n",
    "clf_importance_.columns = {'importance'}\n",
    "bb_columns = pd.DataFrame(bb_data.columns)\n",
    "bb_columns.columns={'feature'}\n",
    "\n",
    "#影响度排序\n",
    "clf_feature_values = pd.concat([bb_columns,clf_importance_],axis=1)\n",
    "clf_feature_values = clf_feature_values.sort_values(by='importance')\n",
    "\n",
    "\n",
    "#影响度非0的特征\n",
    "clf_feature_well = clf_feature_values[clf_feature_values['importance']!=0]\n",
    "clf_feature_well_columns = clf_feature_well['feature'].values\n",
    "clf_feature_well.index = clf_feature_well_columns\n",
    "columns_GBDT = clf_feature_well.index\n",
    "\n",
    "# 测试集提取这些特征，形成新的测试集\n",
    "\n",
    "C_feature = data_bt[columns_GBDT]\n",
    "new_test = data_test[columns_GBDT]\n",
    "C_flag = pd.DataFrame(data_bt['flag'])\n",
    "C_train = pd.concat([C_feature,C_flag],axis=1)\n",
    "\n",
    "#CC = C_feature.fillna(0)\n",
    "#new_test_  = new_test.fillna(0)\n",
    "CC = C_feature\n",
    "new_test_ = new_test\n",
    "\n",
    "#   重要特征训练，训练集交叉验证\n",
    "\n",
    "#from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(CC, C_flag, test_size=0.3, random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=110, learning_rate=1,  max_depth=1, random_state=0)#loss='exponential' mse\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pro = clf.predict_proba(X_test)\n",
    "y_prd = pd.DataFrame(y_pro).iloc[:,1]\n",
    "roc_auc_score(y_test,y_prd)\n",
    "clf.fit(CC,C_flag)\n",
    "b = clf.predict_proba(new_test_)\n",
    "b=pd.DataFrame(b)\n",
    "pro_b = b.iloc[:,1]\n",
    "no = data_test.iloc[:,0]\n",
    "pro = pd.DataFrame(pro_b)\n",
    "no = pd.DataFrame(no)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xg_train = xgb.DMatrix(X_train,label=y_train)\n",
    "xg_test = xgb.DMatrix(X_test,label=y_test)\n",
    "\n",
    "\n",
    "param = {'booster':'gbtree',\n",
    "         'max_depth':10,\n",
    "         'eta':0.1,\n",
    "         'silent':1,\n",
    "         'objective':'binary:logistic',\n",
    "         'eval_metric':'auc',\n",
    "         'subsample': 1,\n",
    "         \"colsample_bytree\": 0.7,\n",
    "         \"min_child_weight\":2,\n",
    "              'gamma':3.1,\n",
    "              'lambda':1,\n",
    "        \"thread\":-1,}\n",
    "num_boost_round = 1500\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'eval')]\n",
    "num_round=15\n",
    "bst = xgb.train(param, xg_train, num_round)\n",
    "preds = bst.predict(xg_test)\n",
    "roc_auc_score(y_test,preds)\n",
    "\n",
    "xg_train = xgb.DMatrix(CC,label=C_flag)\n",
    "xg_test = xgb.DMatrix(new_test_)\n",
    "bst = xgb.train(param, xg_train, num_round)\n",
    "pro = bst.predict(xg_test)\n",
    "\n",
    "xgb_model =xgb.cv(param,xg_train,num_boost_round,nfold=5, early_stopping_rounds=300)#, verbose_eval=True\n",
    "\n",
    "pd.DataFrame(xgb_model)\n",
    "pd.DataFrame(xgb_model)['test-auc-mean'].mean()\n",
    "\n",
    "\n",
    "\n",
    "b=pd.DataFrame(b)\n",
    "\n",
    "XGB_B=[]\n",
    "XGB_B=pd.DataFrame(XGB_B)\n",
    "XGB_B['no'] = no\n",
    "XGB_B['pred'] = pro"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    " GBDT_B_0.587¶"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "A_train = pd.read_csv('../data/A_train.csv')\n",
    "B_train = pd.read_csv('../data/B_train.csv')\n",
    "B_test = pd.read_csv('../data/B_test.csv')\n",
    "\n",
    "A_train['label'] = -1\n",
    "B_train['label'] = 0\n",
    "B_test['label'] = 1\n",
    "B_test['flag'] = np.nan\n",
    "\n",
    "all_data = A_train.append(B_train)\n",
    "all_data = all_data.append(B_test)\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "\n",
    "user_infos = [i for i in all_data.columns if 'UserInfo' in i]\n",
    "product_infos = [i for i in all_data.columns if 'ProductInfo' in i]\n",
    "web_infos = [i for i in all_data.columns if 'WebInfo' in i]\n",
    "\n",
    "all_data = all_data.fillna(10)\n",
    "\n",
    "temp_data = all_data\n",
    "\n",
    "drop_cols_l = ['flag', 'label', 'no']\n",
    "train_x = temp_data[temp_data.label==0].drop(drop_cols_l, axis=1)\n",
    "train_y = temp_data[temp_data.label==0]['flag']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lgb_feature_selection(tr_x, tr_y, model_seed =666,num_rounds = 500):\n",
    "    lgb_tr = lgb.Dataset(tr_x, tr_y)\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'random_state': model_seed}\n",
    "    model = lgb.train(lgb_params, lgb_tr, num_boost_round=num_rounds,verbose_eval=100)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f_model = lgb_feature_selection(train_x.values, train_y.values)\n",
    "lgb.plot_importance(f_model, figsize=(16,8))\n",
    "features_names_im =pd.DataFrame({'feature_name':train_x.columns, 'f_value': f_model.feature_importance()})\n",
    "features_used = features_names_im[features_names_im.f_value>=0.1*features_names_im.f_value.mean()]\n",
    "\n",
    "\n",
    "\n",
    "tr_x = all_data.loc[all_data.label==0,features_used.feature_name.values].values\n",
    "test_x = all_data.loc[all_data.label==1,features_used.feature_name.values].values\n",
    "tr_y = all_data.loc[all_data.label==0, 'flag'].values\n",
    "test_y = all_data[all_data.label==1][['no']]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbdt = GradientBoostingClassifier(n_estimators=400, random_state=666)\n",
    "gbdt.fit(tr_x, tr_y)\n",
    "gbdt_pred = gbdt.predict_proba(test_x)\n",
    "gbdt_pred[:, 1]\n",
    "test_y['pred'] = gbdt_pred[:,1]\n",
    "GBDT_B = test_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " A_B_GDBT\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A_train = pd.read_csv('../data/A_train.csv')\n",
    "B_train = pd.read_csv('../data/B_train.csv')\n",
    "B_test = pd.read_csv('../data/B_test.csv')#//\n",
    "NO = B_test['no']#//\n",
    "\n",
    "\n",
    "# In[67]:\n",
    "\n",
    "B_train_columns = B_train.columns\n",
    "B_null_count_less = []\n",
    "B_null_count_large = []\n",
    "\n",
    "# threshold = 0.63\n",
    "for i in B_train_columns:\n",
    "    if ((B_train[i].isnull().sum()) / len(B_train[i]) <= 0.63):\n",
    "        B_null_count_less.append([i,(B_train[i].isnull().sum()) / len(B_train[i])])\n",
    "    else:\n",
    "        B_null_count_large.append([i,(B_train[i].isnull().sum()) / len(B_train[i])])\n",
    "\n",
    "# len(B_null_count_less) 327\n",
    "\n",
    "# len(B_null_count_large) 164\n",
    "\n",
    "\n",
    "B_test_columns = B_test.columns\n",
    "B_test_count_less = []\n",
    "B_test_count_large = []\n",
    "\n",
    "for i in B_test_columns:\n",
    "    if ((B_test[i].isnull().sum()) / len(B_test[i]) <= 0.63):\n",
    "        B_test_count_less.append([i,(B_test[i].isnull().sum()) / len(B_test[i])])\n",
    "    else:\n",
    "        B_test_count_large.append([i,(B_test[i].isnull().sum()) / len(B_test[i])])\n",
    "\n",
    "\n",
    "A_feature = pd.DataFrame(B_null_count_less).values[:,0]\n",
    "B_feature = pd.DataFrame(B_null_count_less).values[:,0]\n",
    "BT_feature = pd.DataFrame(B_test_count_less).values[:,0]\n",
    "\n",
    "a_data = A_train[A_feature]\n",
    "b_data = B_train[B_feature]\n",
    "bt_data = B_train[BT_feature]\n",
    "\n",
    "a_columns = a_data.columns\n",
    "a_columns = a_columns.sort_values()               #缺失量排序\n",
    "\n",
    "b_columns = b_data.columns   ## B_train columns，多了一个flag\n",
    "b_columns = b_columns.sort_values()\n",
    "\n",
    "bt_columns = bt_data.columns   ## B_test columns\n",
    "bt_columns = bt_columns.sort_values()\n",
    "\n",
    "a_data = A_train[a_columns]\n",
    "b_data = B_train[b_columns]\n",
    "bt_data = B_test[bt_columns]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "b_target = b_data['flag']\n",
    "a_target = a_data['flag']\n",
    "\n",
    "\n",
    "b_data.drop('flag',axis=1,inplace=True)\n",
    "a_data.drop('flag',axis=1,inplace=True)\n",
    "\n",
    "aa_data = a_data.fillna(1)\n",
    "bb_data = b_data.fillna(1)\n",
    "bt_data = bt_data.fillna(1)\n",
    "\n",
    "bb_data.drop('no',axis=1,inplace=True)\n",
    "aa_data.drop('no',axis=1,inplace=True)\n",
    "bt_data.drop('no',axis=1,inplace=True)\n",
    "\n",
    "#-----\n",
    "aa_data = data_at.drop(['no','flag'],axis=1)\n",
    "a_target = data_at['flag']\n",
    "bb_data = data_bt.drop(['no','flag'],axis=1)\n",
    "b_target = data_bt['flag']\n",
    "\n",
    "aa_data = aa_data.fillna(1)\n",
    "bb_data = bb_data.fillna(1)\n",
    "bt_data = bt_data.fillna(1)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1,  max_depth=2,random_state=0)#\n",
    "clf.fit(aa_data, a_target)\n",
    "y_pred = clf.predict_proba(bb_data)\n",
    "y_pred = pd.DataFrame(y_pred).iloc[:,1]\n",
    "roc_auc_score(b_target,y_pred)\n",
    "\n",
    "# clf.fit(aa_data,a_target)\n",
    "clf_importance = clf.feature_importances_\n",
    "clf_importance_ = pd.DataFrame(clf_importance)\n",
    "clf_importance_.columns = {'importance'}\n",
    "bb_columns = pd.DataFrame(bb_data.columns)\n",
    "bb_columns.columns={'feature'}\n",
    "\n",
    "#影响度排序\n",
    "clf_feature_values = pd.concat([bb_columns,clf_importance_],axis=1)\n",
    "# feature_values.columns = {'importance','feature'}\n",
    "clf_feature_values = clf_feature_values.sort_values(by='importance')\n",
    "\n",
    "\n",
    "#影响度非0的特征\n",
    "clf_feature_well = clf_feature_values[clf_feature_values['importance']!=0]\n",
    "clf_feature_well_columns = clf_feature_well['feature'].values\n",
    "clf_feature_well.index = clf_feature_well_columns\n",
    "\n",
    "columns_GBDT = clf_feature_well.index\n",
    "\n",
    "\n",
    "C_feature = A_train[columns_GBDT]\n",
    "D_feature = B_train[columns_GBDT]\n",
    "E_feature = B_test[columns_GBDT]\n",
    "\n",
    "C_feature = data_at[columns_GBDT]\n",
    "D_feature = data_bt[columns_GBDT]\n",
    "E_feature = data_test[columns_GBDT]\n",
    "\n",
    "\n",
    "C_flag = pd.DataFrame(A_train['flag'])\n",
    "D_flag = pd.DataFrame(B_train['flag'])\n",
    "\n",
    "C_82 = C_feature['UserInfo_82']\n",
    "C_82 =pd.DataFrame(C_82.fillna(C_feature['UserInfo_82'].median()))\n",
    "C_82.columns={'new_82'}\n",
    "\n",
    "D_82 = D_feature['UserInfo_82']\n",
    "D_82 =pd.DataFrame(D_82.fillna(E_feature['UserInfo_82'].median())) #用B——test的中位数代替有一点提升\n",
    "D_82.columns={'new_82'}\n",
    "\n",
    "E_82 = E_feature['UserInfo_82']\n",
    "E_82 =pd.DataFrame(E_82.fillna(E_feature['UserInfo_82'].median()))\n",
    "E_82.columns={'new_82'}\n",
    "\n",
    "newC_feature=pd.DataFrame(C_feature['UserInfo_82']*C_feature['UserInfo_222'])\n",
    "newC_feature.columns={'new_feature_1'}\n",
    "\n",
    "newD_feature=pd.DataFrame(D_feature['UserInfo_82']*D_feature['UserInfo_222'])\n",
    "newD_feature.columns={'new_feature_1'}\n",
    "\n",
    "newE_feature=pd.DataFrame(E_feature['UserInfo_82']*E_feature['UserInfo_222'])\n",
    "newE_feature.columns={'new_feature_1'}\n",
    "\n",
    "C_feature = pd.concat([C_feature,C_82],axis = 1)\n",
    "D_feature = pd.concat([D_feature,D_82],axis = 1)\n",
    "E_feature = pd.concat([E_feature,E_82],axis = 1)\n",
    "\n",
    "C_feature = pd.concat([C_feature,newC_feature],axis = 1)\n",
    "D_feature = pd.concat([D_feature,newD_feature],axis = 1)\n",
    "E_feature = pd.concat([E_feature,newE_feature],axis = 1)\n",
    "\n",
    "C_feature = C_feature.fillna(1)\n",
    "D_feature = D_feature.fillna(1)\n",
    "E_feature = E_feature.fillna(1)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=149, learning_rate=0.66,  max_depth=2, random_state=0,max_features=14,min_weight_fraction_leaf=0.11)\n",
    "clf.fit(C_feature, C_flag)\n",
    "\n",
    "y_pred = clf.predict_proba(D_feature)\n",
    "y_pred = pd.DataFrame(y_pred).iloc[:,1]\n",
    "roc_auc_score(D_flag,y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred = clf.predict_proba(E_feature)\n",
    "y_pred = pd.DataFrame(y_pred).iloc[:,1]\n",
    "b=pd.DataFrame(y_pred)\n",
    "\n",
    "no = pd.DataFrame(NO)\n",
    "\n",
    "A_B_GBDT=[]\n",
    "A_B_GBDT=pd.DataFrame(A_B_GBDT)\n",
    "A_B_GBDT['no'] = no\n",
    "A_B_GBDT['pred'] = y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A_B_lgb\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "A_train = pd.read_csv('../data/A_train.csv')\n",
    "B_train = pd.read_csv('../data/B_train.csv')\n",
    "B_test = pd.read_csv('../data/B_test.csv')#//\n",
    "NO = B_test['no']#//\n",
    "\n",
    "B_train_columns = B_train.columns\n",
    "B_null_count_less = []\n",
    "B_null_count_large = []\n",
    "\n",
    "# threshold = 0.63\n",
    "for i in B_train_columns:\n",
    "    if ((B_train[i].isnull().sum()) / len(B_train[i]) <= 0.63):\n",
    "        B_null_count_less.append([i,(B_train[i].isnull().sum()) / len(B_train[i])])\n",
    "    else:\n",
    "        B_null_count_large.append([i,(B_train[i].isnull().sum()) / len(B_train[i])])\n",
    "\n",
    "# len(B_null_count_less) 327\n",
    "\n",
    "# len(B_null_count_large) 164\n",
    "\n",
    "\n",
    "B_test_columns = B_test.columns\n",
    "B_test_count_less = []\n",
    "B_test_count_large = []\n",
    "\n",
    "for i in B_test_columns:\n",
    "    if ((B_test[i].isnull().sum()) / len(B_test[i]) <= 0.63):\n",
    "        B_test_count_less.append([i,(B_test[i].isnull().sum()) / len(B_test[i])])\n",
    "    else:\n",
    "        B_test_count_large.append([i,(B_test[i].isnull().sum()) / len(B_test[i])])\n",
    "\n",
    "A_feature = pd.DataFrame(B_null_count_less).values[:,0]\n",
    "B_feature = pd.DataFrame(B_null_count_less).values[:,0]\n",
    "BT_feature = pd.DataFrame(B_test_count_less).values[:,0]\n",
    "\n",
    "a_data = A_train[A_feature]\n",
    "b_data = B_train[B_feature]\n",
    "bt_data = B_train[BT_feature]\n",
    "\n",
    "a_columns = a_data.columns\n",
    "a_columns = a_columns.sort_values()               #缺失量排序\n",
    "\n",
    "b_columns = b_data.columns   ## B_train columns，多了一个flag\n",
    "b_columns = b_columns.sort_values()\n",
    "\n",
    "bt_columns = bt_data.columns   ## B_test columns\n",
    "bt_columns = bt_columns.sort_values()\n",
    "\n",
    "a_data = A_train[a_columns]\n",
    "b_data = B_train[b_columns]\n",
    "bt_data = B_test[bt_columns]\n",
    "\n",
    "b_target = b_data['flag']\n",
    "a_target = a_data['flag']\n",
    "\n",
    "\n",
    "b_data.drop('flag',axis=1,inplace=True)\n",
    "a_data.drop('flag',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "aa_data = a_data.fillna(1)\n",
    "bb_data = b_data.fillna(1)\n",
    "bt_data = bt_data.fillna(1)\n",
    "\n",
    "bb_data.drop('no',axis=1,inplace=True)\n",
    "aa_data.drop('no',axis=1,inplace=True)\n",
    "bt_data.drop('no',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_train = lgb.Dataset(aa_data,label=a_target)\n",
    "lgb_vd = lgb.Dataset(bb_data,label=b_target)\n",
    "# lgb_test = lgb.Dataset(D_feature)\n",
    "# lgb_vd = lgb.Dataset(vd_x, vd_y, reference=lgb_tr)\n",
    "lgb_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "#     'max_depth':18,\n",
    "#     'feature_fraction':0.85,\n",
    "#     'lambda_l1':1.2,\n",
    "    'random_state': 0}#18     0.85\n",
    "\n",
    "lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=200,verbose_eval=True)\n",
    "\n",
    "lgb_pred = lgb_model.predict(bb_data)\n",
    "\n",
    "roc_auc_score(b_target,lgb_pred)\n",
    "\n",
    "\n",
    "\n",
    "lgb_importance = pd.DataFrame(lgb_model.feature_importance(importance_type=\"split\"))\n",
    "lgb_importance.columns={'importance'}\n",
    "columns = pd.DataFrame(b_columns).iloc[:-2,:]\n",
    "columns.columns={'feature'}\n",
    "lgb_importance = pd.concat([columns,lgb_importance],axis=1)\n",
    "lgb_importance = lgb_importance.sort_values(by='importance')\n",
    "lgb_importance = lgb_importance[lgb_importance['importance']>29].reset_index().drop('index',axis=1)\n",
    "lgb_importance_columns = lgb_importance['feature'].values\n",
    "\n",
    "C_feature = A_train[lgb_importance_columns]\n",
    "D_feature = B_train[lgb_importance_columns]\n",
    "E_feature = B_test[lgb_importance_columns]\n",
    "\n",
    "\n",
    "\n",
    "C_82 = C_feature['UserInfo_82']\n",
    "C_82 =pd.DataFrame(C_82.fillna(C_feature['UserInfo_82'].median()))\n",
    "C_82.columns={'new_82'}\n",
    "\n",
    "D_82 = D_feature['UserInfo_82']\n",
    "D_82 =pd.DataFrame(D_82.fillna(E_feature['UserInfo_82'].median())) #用B——test的中位数代替有一点提升\n",
    "D_82.columns={'new_82'}\n",
    "\n",
    "E_82 = E_feature['UserInfo_82']\n",
    "E_82 =pd.DataFrame(E_82.fillna(E_feature['UserInfo_82'].median()))\n",
    "E_82.columns={'new_82'}\n",
    "\n",
    "newC_feature=pd.DataFrame(C_feature['UserInfo_253']*C_feature['UserInfo_242'])\n",
    "newC_feature.columns={'new_feature_1'}\n",
    "\n",
    "newD_feature=pd.DataFrame(D_feature['UserInfo_253']*D_feature['UserInfo_242'])\n",
    "newD_feature.columns={'new_feature_1'}\n",
    "\n",
    "newE_feature=pd.DataFrame(E_feature['UserInfo_253']*E_feature['UserInfo_242'])\n",
    "newE_feature.columns={'new_feature_1'}\n",
    "\n",
    "\n",
    "C_feature = pd.concat([C_feature,newC_feature],axis = 1)\n",
    "D_feature = pd.concat([D_feature,newD_feature],axis = 1)\n",
    "E_feature = pd.concat([E_feature,newE_feature],axis = 1)\n",
    "\n",
    "C_feature.drop('UserInfo_134',axis=1,inplace=True)\n",
    "D_feature.drop('UserInfo_134',axis=1,inplace=True)\n",
    "E_feature.drop('UserInfo_134',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "C_feature  = C_feature.fillna(1)\n",
    "D_feature = D_feature.fillna(1)\n",
    "E_feature = E_feature.fillna(1)\n",
    "\n",
    "columns = D_feature.columns\n",
    "\n",
    "\n",
    "\n",
    "lgb_params_new = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'max_depth':17,\n",
    "    'feature_fraction':0.80,\n",
    "    'lambda_l1':0.6,\n",
    "#     'scale_pos_weight':1.1,\n",
    "    'random_state': 0}\n",
    "                                                   #17 0.80 0.6 1550 0.591  drop134  new82       253X242    1162\n",
    "lgb_train = lgb.Dataset(C_feature,label=a_target)#17 0.80 0.6 1550 0.589  drop134  new82            1550\n",
    "lgb_vd = lgb.Dataset(D_feature,label=b_target)   #17 0.80 0.6 1469 0.588 drop134\n",
    "lgb_model = lgb.train(lgb_params_new, lgb_train, num_boost_round=2000,verbose_eval=True,valid_sets=lgb_vd, early_stopping_rounds=500)#1256\n",
    "\n",
    "preds = lgb_model.predict(E_feature)\n",
    "no = pd.DataFrame(NO)\n",
    "\n",
    "A_B_LGB=[]\n",
    "A_B_LGB=pd.DataFrame(A_B_LGB)\n",
    "A_B_LGB['no'] = no\n",
    "A_B_LGB['pred'] = preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 融合"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "D=[]\n",
    "D=pd.DataFrame(D)\n",
    "\n",
    "\n",
    "D['no'] = NO\n",
    "D['pred'] = (A_B_GBDT.iloc[:,1]*0.89 + A_B_LGB.iloc[:,1]*0.113)*0.181+ XGB_B.iloc[:,1]*0.415 + GBDT_B.iloc[:,1].values*0.415"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGB_B\n",
    "target_bb = data_bt['flag']\n",
    "\n",
    "bb_data = data_bt.drop('flag',axis=1,inplace=True)\n",
    "bbt_data = data_test.fillna(0)\n",
    "\n",
    "#   GBDT训练 输出（47%以上的特征）   ，训练集划分交叉验证\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validate.train_test_split(bb_data, target_bb, test_size=0.3, random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1,  max_depth=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pro = clf.predict_proba(X_test)\n",
    "y_prd = pd.DataFrame(y_pro).iloc[:,1]\n",
    "roc_auc_score(y_test,y_prd)\n",
    "\n",
    "# 训练所有数据 输出\n",
    "\n",
    "clf.fit(bb_data,target_bb)\n",
    "b = clf.predict_proba(bbt_data)\n",
    "b=pd.DataFrame(b)\n",
    "pro_b = b.iloc[:,1]\n",
    "no = data_test.iloc[:,0]\n",
    "pro = pd.DataFrame(pro_b)\n",
    "no = pd.DataFrame(no)\n",
    "\n",
    "\n",
    "# GBDT 重要特征\n",
    "\n",
    "clf.fit(bb_data,target_bb)\n",
    "clf_importance = clf.feature_importances_\n",
    "clf_importance_ = pd.DataFrame(clf_importance)\n",
    "clf_importance_.columns = {'importance'}\n",
    "bb_columns = pd.DataFrame(bb_data.columns)\n",
    "bb_columns.columns={'feature'}\n",
    "\n",
    "#影响度排序\n",
    "clf_feature_values = pd.concat([bb_columns,clf_importance_],axis=1)\n",
    "clf_feature_values = clf_feature_values.sort_values(by='importance')\n",
    "\n",
    "\n",
    "#影响度非0的特征\n",
    "clf_feature_well = clf_feature_values[clf_feature_values['importance']!=0]\n",
    "clf_feature_well_columns = clf_feature_well['feature'].values\n",
    "clf_feature_well.index = clf_feature_well_columns\n",
    "columns_GBDT = clf_feature_well.index\n",
    "\n",
    "# 测试集提取这些特征，形成新的测试集\n",
    "\n",
    "C_feature = B_train[columns_GBDT]\n",
    "new_test = B_test[columns_GBDT]\n",
    "C_flag = pd.DataFrame(B_train['flag'])\n",
    "C_train = pd.concat([C_feature,C_flag],axis=1)\n",
    "\n",
    "CC = C_feature.fillna(0)\n",
    "new_test_  = new_test.fillna(0)\n",
    "\n",
    "#   重要特征训练，训练集交叉验证\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validate.train_test_split(CC, C_flag, test_size=0.3, random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=110, learning_rate=1,  max_depth=1, random_state=0)#loss='exponential' mse\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pro = clf.predict_proba(X_test)\n",
    "y_prd = pd.DataFrame(y_pro).iloc[:,1]\n",
    "roc_auc_score(y_test,y_prd)\n",
    "clf.fit(CC,C_flag)\n",
    "b = clf.predict_proba(new_test_)\n",
    "b=pd.DataFrame(b)\n",
    "pro_b = b.iloc[:,1]\n",
    "no = B_test.iloc[:,0]\n",
    "pro = pd.DataFrame(pro_b)\n",
    "no = pd.DataFrame(no)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xg_train = xgb.DMatrix(X_train,label=y_train)\n",
    "xg_test = xgb.DMatrix(X_test,label=y_test)\n",
    "\n",
    "\n",
    "param = {'booster':'gbtree',\n",
    "         'max_depth':10,\n",
    "         'eta':0.1,\n",
    "         'silent':1,\n",
    "         'objective':'binary:logistic',\n",
    "         'eval_metric':'auc',\n",
    "         'subsample': 1,\n",
    "         \"colsample_bytree\": 0.7,\n",
    "         \"min_child_weight\":2,\n",
    "              'gamma':3.1,\n",
    "              'lambda':1,\n",
    "        \"thread\":-1,}\n",
    "num_boost_round = 1500\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'eval')]\n",
    "num_round=15\n",
    "bst = xgb.train(param, xg_train, num_round)\n",
    "preds = bst.predict(xg_test)\n",
    "roc_auc_score(y_test,preds)\n",
    "\n",
    "xg_train = xgb.DMatrix(CC,label=C_flag)\n",
    "xg_test = xgb.DMatrix(new_test_)\n",
    "bst = xgb.train(param, xg_train, num_round)\n",
    "pro = bst.predict(xg_test)\n",
    "\n",
    "xgb_model =xgb.cv(param,xg_train,num_boost_round,nfold=5, early_stopping_rounds=300)#, verbose_eval=True\n",
    "\n",
    "pd.DataFrame(xgb_model)\n",
    "pd.DataFrame(xgb_model)['test-auc-mean'].mean()\n",
    "\n",
    "\n",
    "\n",
    "b=pd.DataFrame(b)\n",
    "\n",
    "no = pd.DataFrame(NO)\n",
    "\n",
    "XGB_B=[]\n",
    "XGB_B=pd.DataFrame(XGB_B)\n",
    "XGB_B['no'] = no\n",
    "XGB_B['pred'] = pro"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# XGB_B\n",
    "target_bb = data_bt['flag']\n",
    "\n",
    "bb_data = data_bt.drop('flag',axis=1,inplace=True)\n",
    "bbt_data = data_test.fillna(0)\n",
    "\n",
    "#   GBDT训练 输出（47%以上的特征）   ，训练集划分交叉验证\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validate.train_test_split(bb_data, target_bb, test_size=0.3, random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1,  max_depth=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pro = clf.predict_proba(X_test)\n",
    "y_prd = pd.DataFrame(y_pro).iloc[:,1]\n",
    "roc_auc_score(y_test,y_prd)\n",
    "\n",
    "# 训练所有数据 输出\n",
    "\n",
    "clf.fit(bb_data,target_bb)\n",
    "b = clf.predict_proba(bbt_data)\n",
    "b=pd.DataFrame(b)\n",
    "pro_b = b.iloc[:,1]\n",
    "no = data_test.iloc[:,0]\n",
    "pro = pd.DataFrame(pro_b)\n",
    "no = pd.DataFrame(no)\n",
    "\n",
    "\n",
    "# GBDT 重要特征\n",
    "\n",
    "clf.fit(bb_data,target_bb)\n",
    "clf_importance = clf.feature_importances_\n",
    "clf_importance_ = pd.DataFrame(clf_importance)\n",
    "clf_importance_.columns = {'importance'}\n",
    "bb_columns = pd.DataFrame(bb_data.columns)\n",
    "bb_columns.columns={'feature'}\n",
    "\n",
    "#影响度排序\n",
    "clf_feature_values = pd.concat([bb_columns,clf_importance_],axis=1)\n",
    "clf_feature_values = clf_feature_values.sort_values(by='importance')\n",
    "\n",
    "\n",
    "#影响度非0的特征\n",
    "clf_feature_well = clf_feature_values[clf_feature_values['importance']!=0]\n",
    "clf_feature_well_columns = clf_feature_well['feature'].values\n",
    "clf_feature_well.index = clf_feature_well_columns\n",
    "columns_GBDT = clf_feature_well.index\n",
    "\n",
    "# 测试集提取这些特征，形成新的测试集\n",
    "\n",
    "C_feature = B_train[columns_GBDT]\n",
    "new_test = B_test[columns_GBDT]\n",
    "C_flag = pd.DataFrame(B_train['flag'])\n",
    "C_train = pd.concat([C_feature,C_flag],axis=1)\n",
    "\n",
    "CC = C_feature.fillna(0)\n",
    "new_test_  = new_test.fillna(0)\n",
    "\n",
    "#   重要特征训练，训练集交叉验证\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validate.train_test_split(CC, C_flag, test_size=0.3, random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=110, learning_rate=1,  max_depth=1, random_state=0)#loss='exponential' mse\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pro = clf.predict_proba(X_test)\n",
    "y_prd = pd.DataFrame(y_pro).iloc[:,1]\n",
    "roc_auc_score(y_test,y_prd)\n",
    "clf.fit(CC,C_flag)\n",
    "b = clf.predict_proba(new_test_)\n",
    "b=pd.DataFrame(b)\n",
    "pro_b = b.iloc[:,1]\n",
    "no = B_test.iloc[:,0]\n",
    "pro = pd.DataFrame(pro_b)\n",
    "no = pd.DataFrame(no)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xg_train = xgb.DMatrix(X_train,label=y_train)\n",
    "xg_test = xgb.DMatrix(X_test,label=y_test)\n",
    "\n",
    "\n",
    "param = {'booster':'gbtree',\n",
    "         'max_depth':10,\n",
    "         'eta':0.1,\n",
    "         'silent':1,\n",
    "         'objective':'binary:logistic',\n",
    "         'eval_metric':'auc',\n",
    "         'subsample': 1,\n",
    "         \"colsample_bytree\": 0.7,\n",
    "         \"min_child_weight\":2,\n",
    "              'gamma':3.1,\n",
    "              'lambda':1,\n",
    "        \"thread\":-1,}\n",
    "num_boost_round = 1500\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'eval')]\n",
    "num_round=15\n",
    "bst = xgb.train(param, xg_train, num_round)\n",
    "preds = bst.predict(xg_test)\n",
    "roc_auc_score(y_test,preds)\n",
    "\n",
    "xg_train = xgb.DMatrix(CC,label=C_flag)\n",
    "xg_test = xgb.DMatrix(new_test_)\n",
    "bst = xgb.train(param, xg_train, num_round)\n",
    "pro = bst.predict(xg_test)\n",
    "\n",
    "xgb_model =xgb.cv(param,xg_train,num_boost_round,nfold=5, early_stopping_rounds=300)#, verbose_eval=True\n",
    "\n",
    "pd.DataFrame(xgb_model)\n",
    "pd.DataFrame(xgb_model)['test-auc-mean'].mean()\n",
    "\n",
    "\n",
    "\n",
    "b=pd.DataFrame(b)\n",
    "\n",
    "no = pd.DataFrame(NO)\n",
    "\n",
    "XGB_B=[]\n",
    "XGB_B=pd.DataFrame(XGB_B)\n",
    "XGB_B['no'] = no\n",
    "XGB_B['pred'] = pro"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "# coding=utf-8\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}